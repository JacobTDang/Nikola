{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "iebRHebLGoeR",
        "outputId": "2b7e5517-4a99-4cdc-ec91-64ad8a701939"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-26-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class nikola(nn.Module):\n",
        "  def __init__(self):\n",
        "    # calls constructor of parent class to properly set everything up\n",
        "    super().__init__()\n",
        "\n",
        "    # sequential is a container that holds a series of layers that data will pass through\n",
        "    self.conv_layers = nn.Sequential(\n",
        "    # 2d convolution layer\n",
        "    nn.Conv2d(in_channels=3, out_channels=24, kernel_size=5, stride=2),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(in_channels=24, out_channels=36, kernel_size=5, stride=2),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(in_channels=36, out_channels=48, kernel_size=3, stride=2),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(in_channels=48, out_channels=64, kernel_size=3, stride=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(in_channels=64, out_channels= 64, kernel_size=3, stride=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Flatten()\n",
        "    )\n",
        "\n",
        "    self.dense_layers = nn.Sequential(\n",
        "      nn.Linear(in_features=8064, out_features=100),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(in_features=100, out_features=50),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(in_features=50, out_features=10),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(in_features=10, out_features=2)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    # 'x' is the input image tensor\n",
        "\n",
        "    # image goes through all convolution layers\n",
        "    x = self.conv_layers(x)\n",
        "    # result from convolution layer is then passed through all the dense layers\n",
        "    x = self.dense_layers(x)\n",
        "    # return the results (steering and throttle)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "0FQNrZNRH1-j"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# nikola class is defined above\n",
        "model = nikola()\n",
        "\n",
        "# Create a dummy input tensor to test the output size\n",
        "# Shape is: (batch_size, channels, height, width)\n",
        "# Use the image size you plan to train on (e.g., 120x160)\n",
        "dummy_input = torch.randn(1, 3, 120, 160)\n",
        "\n",
        "# Pass the dummy input through only the convolutional part\n",
        "conv_output = model.conv_layers(dummy_input)\n",
        "\n",
        "# Print the flattened size\n",
        "print(f\"Output shape after convolutional layers: {conv_output.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUU6sOhwIfuD",
        "outputId": "594d166b-4b83-4c99-b4bf-b4ef08212099"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape after convolutional layers: torch.Size([1, 8064])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "class UdacityDataset(Dataset):\n",
        "    def __init__(self, data_dir, csv_filename='driving_log.csv'):\n",
        "        self.data_dir = data_dir\n",
        "        csv_path = os.path.join(self.data_dir, csv_filename)\n",
        "\n",
        "        # Define the column names since the file has no header\n",
        "        column_names = ['centercam', 'leftcam', 'rightcam', 'steering', 'throttle', 'brake', 'speed']\n",
        "        self.log = pd.read_csv(csv_path, names=column_names)\n",
        "\n",
        "        # Define the transformation pipeline\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Resize((120, 160), antialias=True),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.log)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.log.iloc[idx]\n",
        "\n",
        "        # Use the correct column names we just defined above\n",
        "        steering = row['steering']\n",
        "        throttle = row['throttle']\n",
        "        labels = torch.tensor([steering, throttle], dtype=torch.float32)\n",
        "\n",
        "        # Get the original image path from the 'centercam' column\n",
        "        original_path = row['centercam']\n",
        "\n",
        "        filename = original_path.split('\\\\')[-1]\n",
        "        correct_path = os.path.join(self.data_dir, 'IMG', filename.strip())\n",
        "\n",
        "        # Load the image and handle potential errors\n",
        "        image = cv2.imread(correct_path)\n",
        "\n",
        "        if image is None:\n",
        "            print(f\"Warning: Could not load image at {correct_path}. Skipping this data point.\")\n",
        "            return None  # Return None for invalid data points\n",
        "\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        image = self.transform(image)\n",
        "\n",
        "        return image, labels"
      ],
      "metadata": {
        "id": "nyd9Xs-dKO6y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# Custom collate function to filter out None values\n",
        "def collate_fn_skip_none(batch):\n",
        "    # Filter out None values\n",
        "    batch = [item for item in batch if item is not None]\n",
        "    if not batch: # Return None if batch is empty after filtering\n",
        "        return None\n",
        "    return torch.utils.data.dataloader.default_collate(batch)\n",
        "\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/Nikola Data/Udacity/self_driving_car_dataset_make'\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "epoch = 5\n",
        "\n",
        "# load and splitting the data\n",
        "full_dataset = UdacityDataset(data_dir=data_dir)\n",
        "print(f\"Actual CSV Columns: {full_dataset.log.columns.values}\")\n",
        "\n",
        "train_size = int(0.8*len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "print(f\"Data loaded: {len(train_dataset)} training samples, {len(val_dataset)} validation samples.\")\n",
        "\n",
        "# create the dataloaders, using the custom collate function\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_skip_none)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_skip_none)\n",
        "\n",
        "# Intialize model, loss func, and optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = nikola().to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "print(\"Setup complete. Ready to start training loop.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72bWMOt7WMnk",
        "outputId": "c88e0965-20df-4a6b-ca83-3c13f8095b02"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual CSV Columns: ['centercam' 'leftcam' 'rightcam' 'steering' 'throttle' 'brake' 'speed']\n",
            "Data loaded: 3144 training samples, 786 validation samples.\n",
            "Using device: cuda\n",
            "Setup complete. Ready to start training loop.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # --- Training Loop ---\n",
        "    best_val_loss = float('inf')\n",
        "    EPOCHS = 5 # You can increase this number for longer training\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        # --- Training Phase ---\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            # Move data to the GPU\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass (backpropagation)\n",
        "            loss.backward()\n",
        "\n",
        "            # Update weights\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "\n",
        "        # --- Validation Phase ---\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                running_val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = running_val_loss / len(val_loader)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{EPOCHS}] -> Train Loss: {avg_train_loss:.4f} | Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # --- Save the Best Model ---\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            print(\"New best model saved!\")\n",
        "\n",
        "    print(\"Finished Training!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK1rO6mps5Zt",
        "outputId": "6b36e23b-4c7b-4b46-9924-6871be61bb92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Could not load image at /content/drive/MyDrive/Nikola Data/Udacity/self_driving_car_dataset_make/IMG/center_2022_04_10_12_27_50_362.jpg. Skipping this data point.\n",
            "Warning: Could not load image at /content/drive/MyDrive/Nikola Data/Udacity/self_driving_car_dataset_make/IMG/center_2022_04_10_12_27_50_429.jpg. Skipping this data point.\n",
            "Warning: Could not load image at /content/drive/MyDrive/Nikola Data/Udacity/self_driving_car_dataset_make/IMG/center_2022_04_10_12_27_50_567.jpg. Skipping this data point.\n",
            "Warning: Could not load image at /content/drive/MyDrive/Nikola Data/Udacity/self_driving_car_dataset_make/IMG/center_2022_04_10_12_27_50_084.jpg. Skipping this data point.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n45XTftWtDeW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}